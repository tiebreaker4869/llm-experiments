{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "363fbac8",
   "metadata": {},
   "source": [
    "### FFN, norm in LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f80c07dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fed5eb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim: int, hidden_dim: int):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.w1 = nn.Linear(self.dim, self.hidden_dim)\n",
    "        self.w2 = nn.Linear(self.dim, self.hidden_dim)\n",
    "        self.w3 = nn.Linear(self.hidden_dim, self.dim)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.w3(F.silu(self.w1(x)) * self.w2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaf72859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0324,  0.0382,  0.0036,  ...,  0.0129,  0.0053, -0.0337],\n",
       "          [-0.0264,  0.0280,  0.0458,  ...,  0.0265, -0.0198, -0.0292],\n",
       "          [-0.0722,  0.0506, -0.0119,  ...,  0.0156,  0.0095,  0.0085],\n",
       "          ...,\n",
       "          [-0.0248,  0.0268,  0.0094,  ..., -0.0036, -0.0083, -0.0375],\n",
       "          [-0.0650,  0.0223,  0.0308,  ..., -0.0044, -0.0198,  0.0015],\n",
       "          [-0.0594,  0.0161, -0.0010,  ...,  0.0190, -0.0027, -0.0419]],\n",
       " \n",
       "         [[-0.0659,  0.0358,  0.0241,  ...,  0.0315,  0.0461, -0.0354],\n",
       "          [-0.0520,  0.0563, -0.0149,  ...,  0.0064, -0.0176, -0.0308],\n",
       "          [-0.0007,  0.0532,  0.0424,  ..., -0.0126,  0.0382, -0.0633],\n",
       "          ...,\n",
       "          [-0.0668,  0.0489,  0.0088,  ...,  0.0140,  0.0172, -0.0055],\n",
       "          [-0.0122,  0.0284,  0.0109,  ...,  0.0213,  0.0162, -0.0819],\n",
       "          [-0.0381,  0.0546,  0.0286,  ...,  0.0238,  0.0037, -0.0294]],\n",
       " \n",
       "         [[-0.0947,  0.1174, -0.0368,  ..., -0.0188,  0.0275,  0.0025],\n",
       "          [-0.0315,  0.0335,  0.0528,  ..., -0.0159, -0.0003, -0.0448],\n",
       "          [-0.0456,  0.0559, -0.0031,  ...,  0.0202, -0.0230, -0.0938],\n",
       "          ...,\n",
       "          [-0.0496,  0.0176,  0.0049,  ...,  0.0300,  0.0056, -0.0118],\n",
       "          [-0.0267,  0.0402, -0.0135,  ..., -0.0100, -0.0147, -0.0257],\n",
       "          [-0.0405,  0.0484,  0.0177,  ..., -0.0330, -0.0101, -0.0131]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0568,  0.0491,  0.0009,  ...,  0.0385, -0.0057, -0.0575],\n",
       "          [-0.0215,  0.0336,  0.0481,  ..., -0.0010,  0.0126, -0.0439],\n",
       "          [-0.0124,  0.0294,  0.0275,  ..., -0.0305, -0.0168,  0.0099],\n",
       "          ...,\n",
       "          [-0.0497,  0.0520,  0.0262,  ...,  0.0110,  0.0286, -0.0395],\n",
       "          [-0.0572,  0.0315,  0.0318,  ...,  0.0185,  0.0021, -0.0630],\n",
       "          [-0.0147,  0.0155,  0.0151,  ...,  0.0293,  0.0070,  0.0007]],\n",
       " \n",
       "         [[-0.0640,  0.0399,  0.0079,  ...,  0.0349,  0.0337, -0.0270],\n",
       "          [-0.0892,  0.0330, -0.0239,  ...,  0.0123,  0.0016, -0.0519],\n",
       "          [-0.0908,  0.0397,  0.0073,  ...,  0.0070, -0.0050, -0.0192],\n",
       "          ...,\n",
       "          [-0.0318,  0.0353, -0.0109,  ...,  0.0060, -0.0055, -0.0400],\n",
       "          [-0.0351,  0.0584,  0.0348,  ..., -0.0233, -0.0105, -0.0066],\n",
       "          [-0.0452,  0.0436,  0.0368,  ...,  0.0254, -0.0246, -0.0477]],\n",
       " \n",
       "         [[-0.0851,  0.0274,  0.0127,  ...,  0.0237,  0.0352, -0.0118],\n",
       "          [-0.0388,  0.0896,  0.0261,  ...,  0.0107, -0.0071,  0.0010],\n",
       "          [-0.0436,  0.0151,  0.0356,  ...,  0.0075,  0.0025, -0.0409],\n",
       "          ...,\n",
       "          [-0.0566,  0.0375,  0.0256,  ...,  0.0234,  0.0111, -0.0312],\n",
       "          [-0.0461, -0.0042,  0.0298,  ...,  0.0087,  0.0160, -0.0141],\n",
       "          [-0.0430,  0.0347, -0.0014,  ..., -0.0107, -0.0002, -0.0443]]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " torch.Size([32, 16, 64]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn = FeedForward(64, 256)\n",
    "input = torch.rand((32, 16, 64))\n",
    "out = ffn(input)\n",
    "out, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e60bb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-8):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.dim = dim\n",
    "        self.weight = nn.Parameter(torch.ones((self.dim, )))\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x / torch.mean(x.pow(2), dim=-1, keepdim=True) * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "840702d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.7381, 2.1965, 0.8053,  ..., 2.0828, 1.3132, 2.5500],\n",
       "          [0.9285, 0.9444, 3.1861,  ..., 1.0493, 0.5654, 1.6959],\n",
       "          [2.2777, 1.6707, 0.6436,  ..., 0.0892, 0.4929, 2.4850],\n",
       "          ...,\n",
       "          [2.4916, 3.3569, 2.0914,  ..., 1.8038, 2.1059, 2.1821],\n",
       "          [1.7375, 1.9472, 0.3976,  ..., 0.1947, 2.4472, 1.9364],\n",
       "          [2.2786, 2.8348, 1.0656,  ..., 0.0505, 1.2916, 0.8681]],\n",
       " \n",
       "         [[2.2399, 0.1426, 2.0694,  ..., 0.8386, 2.4350, 0.2784],\n",
       "          [0.9224, 2.5744, 0.9108,  ..., 2.2597, 2.2281, 0.1601],\n",
       "          [1.3694, 2.2160, 1.0938,  ..., 1.4120, 0.9671, 2.8722],\n",
       "          ...,\n",
       "          [2.2928, 0.4488, 2.3437,  ..., 0.0244, 0.3041, 1.3093],\n",
       "          [0.5280, 0.7425, 2.4410,  ..., 2.1261, 0.5390, 1.0719],\n",
       "          [1.5403, 2.3955, 2.7408,  ..., 0.2198, 1.4365, 0.9891]],\n",
       " \n",
       "         [[1.1112, 0.4813, 1.0927,  ..., 0.3830, 1.5777, 1.1195],\n",
       "          [1.2439, 0.6443, 2.7290,  ..., 1.6907, 2.7991, 0.0906],\n",
       "          [2.2355, 2.0358, 2.2947,  ..., 2.1838, 1.6190, 1.5779],\n",
       "          ...,\n",
       "          [1.1576, 1.3335, 0.7688,  ..., 2.0712, 1.5976, 1.8584],\n",
       "          [2.0631, 2.4275, 0.1785,  ..., 1.2120, 0.7639, 0.5131],\n",
       "          [0.3378, 0.8406, 0.7168,  ..., 1.3045, 0.5060, 0.6515]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1.6519, 1.2228, 1.4930,  ..., 0.8125, 1.9221, 1.8247],\n",
       "          [2.3122, 1.3184, 2.2540,  ..., 1.1413, 0.8274, 2.4075],\n",
       "          [1.8856, 1.9399, 1.4453,  ..., 0.6881, 0.2526, 0.4787],\n",
       "          ...,\n",
       "          [1.7348, 0.2046, 0.8887,  ..., 0.5787, 2.2075, 1.9037],\n",
       "          [3.0323, 0.4561, 2.0912,  ..., 1.8590, 0.7935, 3.3294],\n",
       "          [0.3626, 0.8560, 2.1222,  ..., 2.1509, 2.4255, 1.5639]],\n",
       " \n",
       "         [[0.5467, 2.6195, 0.4979,  ..., 0.2513, 2.4450, 0.1204],\n",
       "          [1.9698, 2.3982, 2.1291,  ..., 2.2376, 0.4918, 1.6165],\n",
       "          [0.5517, 1.3991, 0.0348,  ..., 0.1626, 2.0661, 1.9463],\n",
       "          ...,\n",
       "          [2.5349, 2.8545, 0.5263,  ..., 1.1848, 1.4021, 1.2282],\n",
       "          [0.7486, 0.9431, 0.3065,  ..., 2.9483, 2.5606, 2.0161],\n",
       "          [1.6508, 1.8058, 1.3690,  ..., 0.9397, 0.3262, 1.2434]],\n",
       " \n",
       "         [[2.4014, 1.6176, 0.3095,  ..., 1.6236, 0.0495, 0.9699],\n",
       "          [2.7665, 2.4162, 0.9345,  ..., 1.2691, 1.3511, 0.6085],\n",
       "          [1.2315, 1.8561, 0.4351,  ..., 0.5977, 2.0811, 1.4062],\n",
       "          ...,\n",
       "          [0.2832, 1.6666, 0.8276,  ..., 1.0230, 2.3643, 2.1423],\n",
       "          [1.0587, 0.7215, 1.5809,  ..., 0.6513, 1.0984, 0.1455],\n",
       "          [1.9349, 1.4700, 2.1644,  ..., 0.8955, 1.9633, 1.7801]]],\n",
       "        grad_fn=<MulBackward0>),\n",
       " torch.Size([16, 64, 32]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmsnorm = RMSNorm(32)\n",
    "input = torch.rand((16, 64, 32))\n",
    "out = rmsnorm(input)\n",
    "out, out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
