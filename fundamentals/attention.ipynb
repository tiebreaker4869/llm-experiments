{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a5273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465468ba",
   "metadata": {},
   "source": [
    "## Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4125a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_dim: int, n_heads: int):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_dim = n_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = self.n_dim // self.n_heads\n",
    "        self.q_linear = nn.Linear(n_dim, n_dim)\n",
    "        self.k_linear = nn.Linear(n_dim, n_dim)\n",
    "        self.v_linear = nn.Linear(n_dim, n_dim)\n",
    "        self.out_linear = nn.Linear(n_dim, n_dim)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor = None):\n",
    "        bs = x.size()[0]\n",
    "        q, k, v = self.q_linear(x), self.k_linear(x), self.v_linear(x)\n",
    "        q, k, v = self.split_head(q), self.split_head(k), self.split_head(v)\n",
    "        scores = torch.matmul(q, k.transpose(-1, -2)) / self.head_dim ** 0.5 # (b, n, s, s)\n",
    "        if attention_mask is not None:\n",
    "            scores.masked_fill_(attention_mask == 0, float('-inf'))\n",
    "        scores = torch.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(scores, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(bs, -1, self.n_dim)\n",
    "        out = self.out_linear(out)\n",
    "        return out\n",
    "    \n",
    "    def split_head(self, x: torch.Tensor):\n",
    "        bs = x.size()[0]\n",
    "        return x.view(bs, -1, self.n_heads, self.head_dim).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aa7ad6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2974, -0.1306,  0.0310,  ...,  0.0902,  0.1056, -0.0551],\n",
       "         [-0.2978, -0.1315,  0.0307,  ...,  0.0893,  0.1045, -0.0551],\n",
       "         [-0.2980, -0.1326,  0.0310,  ...,  0.0905,  0.1049, -0.0531],\n",
       "         ...,\n",
       "         [-0.2976, -0.1325,  0.0303,  ...,  0.0896,  0.1050, -0.0536],\n",
       "         [-0.2970, -0.1319,  0.0312,  ...,  0.0891,  0.1049, -0.0551],\n",
       "         [-0.2962, -0.1325,  0.0296,  ...,  0.0890,  0.1056, -0.0545]],\n",
       "\n",
       "        [[-0.2957, -0.1326,  0.0064,  ...,  0.0912,  0.1377, -0.0308],\n",
       "         [-0.2947, -0.1326,  0.0071,  ...,  0.0880,  0.1367, -0.0319],\n",
       "         [-0.2959, -0.1317,  0.0072,  ...,  0.0897,  0.1383, -0.0318],\n",
       "         ...,\n",
       "         [-0.2951, -0.1347,  0.0057,  ...,  0.0880,  0.1391, -0.0294],\n",
       "         [-0.2944, -0.1360,  0.0057,  ...,  0.0877,  0.1387, -0.0287],\n",
       "         [-0.2929, -0.1330,  0.0068,  ...,  0.0883,  0.1396, -0.0303]],\n",
       "\n",
       "        [[-0.3553, -0.1200,  0.0087,  ...,  0.0742,  0.1008, -0.0897],\n",
       "         [-0.3562, -0.1199,  0.0106,  ...,  0.0744,  0.0991, -0.0894],\n",
       "         [-0.3557, -0.1193,  0.0103,  ...,  0.0744,  0.0990, -0.0897],\n",
       "         ...,\n",
       "         [-0.3559, -0.1195,  0.0109,  ...,  0.0745,  0.0982, -0.0899],\n",
       "         [-0.3560, -0.1211,  0.0096,  ...,  0.0743,  0.1000, -0.0890],\n",
       "         [-0.3558, -0.1208,  0.0086,  ...,  0.0751,  0.0992, -0.0897]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.3290, -0.1673,  0.0067,  ...,  0.1264,  0.1415, -0.0499],\n",
       "         [-0.3296, -0.1672,  0.0072,  ...,  0.1269,  0.1414, -0.0501],\n",
       "         [-0.3314, -0.1685,  0.0068,  ...,  0.1268,  0.1387, -0.0496],\n",
       "         ...,\n",
       "         [-0.3295, -0.1671,  0.0068,  ...,  0.1252,  0.1409, -0.0509],\n",
       "         [-0.3288, -0.1669,  0.0072,  ...,  0.1252,  0.1399, -0.0498],\n",
       "         [-0.3289, -0.1665,  0.0069,  ...,  0.1259,  0.1410, -0.0509]],\n",
       "\n",
       "        [[-0.3300, -0.1373, -0.0514,  ...,  0.0757,  0.1202, -0.0664],\n",
       "         [-0.3308, -0.1374, -0.0512,  ...,  0.0763,  0.1185, -0.0665],\n",
       "         [-0.3298, -0.1379, -0.0510,  ...,  0.0757,  0.1188, -0.0664],\n",
       "         ...,\n",
       "         [-0.3303, -0.1375, -0.0513,  ...,  0.0742,  0.1195, -0.0678],\n",
       "         [-0.3307, -0.1378, -0.0519,  ...,  0.0758,  0.1200, -0.0669],\n",
       "         [-0.3300, -0.1379, -0.0517,  ...,  0.0767,  0.1191, -0.0668]],\n",
       "\n",
       "        [[-0.2780, -0.1802,  0.0231,  ...,  0.0381,  0.0973, -0.0798],\n",
       "         [-0.2779, -0.1794,  0.0244,  ...,  0.0395,  0.0989, -0.0795],\n",
       "         [-0.2780, -0.1794,  0.0226,  ...,  0.0380,  0.0984, -0.0806],\n",
       "         ...,\n",
       "         [-0.2787, -0.1785,  0.0235,  ...,  0.0392,  0.0975, -0.0809],\n",
       "         [-0.2780, -0.1786,  0.0239,  ...,  0.0388,  0.0987, -0.0802],\n",
       "         [-0.2777, -0.1782,  0.0250,  ...,  0.0406,  0.0988, -0.0794]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = MultiHeadAttention(32, 4)\n",
    "input = torch.rand((32, 16, 32))\n",
    "out = mha(input)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0701196b",
   "metadata": {},
   "source": [
    "## Multi Query Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89bb1980",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    def __init__(self, n_dim: int, n_heads: int):\n",
    "        super(MultiQueryAttention, self).__init__()\n",
    "        self.n_dim = n_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = self.n_dim // self.n_heads\n",
    "        self.q_linear = nn.Linear(self.n_dim, self.n_dim)\n",
    "        self.k_linear = nn.Linear(self.n_dim, self.head_dim) # different with MHA\n",
    "        self.v_linear = nn.Linear(self.n_dim, self.head_dim) # different with MHA\n",
    "        self.out_linear = nn.Linear(self.n_dim, self.n_dim)\n",
    "    \n",
    "    def split_head(self, x: torch.Tensor, n: int = None):\n",
    "        bs = x.size()[0]\n",
    "        if n is not None:\n",
    "            return x.view(bs, -1, n, self.head_dim).transpose(1, 2)\n",
    "        else:\n",
    "            return x.view(bs, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor = None):\n",
    "        bs = x.size()[0]\n",
    "        q, k, v = self.q_linear(x), self.k_linear(x), self.v_linear(x)\n",
    "        q, k, v = self.split_head(q), self.split_head(k, 1), self.split_head(v, 1)\n",
    "        k = k.expand(-1, self.n_heads, -1, -1)\n",
    "        v = v.expand(-1, self.n_heads, -1, -1)\n",
    "        scores = torch.matmul(q, k.transpose(-1, -2)) / self.head_dim ** 0.5\n",
    "        if attention_mask is not None:\n",
    "            scores.masked_fill_(attention_mask == 0, float('-inf'))\n",
    "        scores = torch.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(scores, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(bs, -1, self.n_dim)\n",
    "        out = self.out_linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26003e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1611, -0.2697,  0.1478,  ..., -0.1122,  0.0269, -0.0183],\n",
       "         [ 0.1612, -0.2703,  0.1490,  ..., -0.1133,  0.0281, -0.0196],\n",
       "         [ 0.1592, -0.2689,  0.1486,  ..., -0.1113,  0.0278, -0.0192],\n",
       "         ...,\n",
       "         [ 0.1600, -0.2695,  0.1490,  ..., -0.1119,  0.0273, -0.0190],\n",
       "         [ 0.1612, -0.2701,  0.1485,  ..., -0.1125,  0.0271, -0.0187],\n",
       "         [ 0.1608, -0.2704,  0.1494,  ..., -0.1120,  0.0258, -0.0192]],\n",
       "\n",
       "        [[ 0.1663, -0.2458,  0.1352,  ..., -0.1233,  0.0326, -0.0444],\n",
       "         [ 0.1665, -0.2458,  0.1347,  ..., -0.1230,  0.0330, -0.0437],\n",
       "         [ 0.1661, -0.2460,  0.1356,  ..., -0.1226,  0.0324, -0.0449],\n",
       "         ...,\n",
       "         [ 0.1664, -0.2463,  0.1354,  ..., -0.1241,  0.0328, -0.0446],\n",
       "         [ 0.1666, -0.2458,  0.1357,  ..., -0.1233,  0.0326, -0.0452],\n",
       "         [ 0.1660, -0.2450,  0.1344,  ..., -0.1229,  0.0324, -0.0456]],\n",
       "\n",
       "        [[ 0.1519, -0.2341,  0.1297,  ..., -0.1129,  0.0252, -0.0425],\n",
       "         [ 0.1524, -0.2349,  0.1302,  ..., -0.1132,  0.0247, -0.0429],\n",
       "         [ 0.1516, -0.2343,  0.1305,  ..., -0.1128,  0.0245, -0.0431],\n",
       "         ...,\n",
       "         [ 0.1514, -0.2344,  0.1305,  ..., -0.1121,  0.0245, -0.0427],\n",
       "         [ 0.1524, -0.2344,  0.1299,  ..., -0.1126,  0.0247, -0.0428],\n",
       "         [ 0.1521, -0.2341,  0.1299,  ..., -0.1123,  0.0250, -0.0428]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.1662, -0.2584,  0.1321,  ..., -0.1183,  0.0385, -0.0401],\n",
       "         [ 0.1662, -0.2578,  0.1324,  ..., -0.1186,  0.0389, -0.0400],\n",
       "         [ 0.1665, -0.2585,  0.1317,  ..., -0.1184,  0.0399, -0.0393],\n",
       "         ...,\n",
       "         [ 0.1659, -0.2583,  0.1324,  ..., -0.1185,  0.0378, -0.0396],\n",
       "         [ 0.1651, -0.2581,  0.1316,  ..., -0.1177,  0.0389, -0.0388],\n",
       "         [ 0.1665, -0.2575,  0.1320,  ..., -0.1173,  0.0390, -0.0399]],\n",
       "\n",
       "        [[ 0.1621, -0.2526,  0.1400,  ..., -0.1154,  0.0236, -0.0352],\n",
       "         [ 0.1619, -0.2520,  0.1394,  ..., -0.1155,  0.0240, -0.0358],\n",
       "         [ 0.1624, -0.2520,  0.1387,  ..., -0.1160,  0.0237, -0.0364],\n",
       "         ...,\n",
       "         [ 0.1626, -0.2530,  0.1396,  ..., -0.1157,  0.0243, -0.0349],\n",
       "         [ 0.1622, -0.2522,  0.1394,  ..., -0.1154,  0.0243, -0.0356],\n",
       "         [ 0.1627, -0.2519,  0.1396,  ..., -0.1155,  0.0238, -0.0364]],\n",
       "\n",
       "        [[ 0.1674, -0.2729,  0.1478,  ..., -0.1139,  0.0241, -0.0175],\n",
       "         [ 0.1693, -0.2739,  0.1479,  ..., -0.1139,  0.0227, -0.0175],\n",
       "         [ 0.1682, -0.2735,  0.1481,  ..., -0.1134,  0.0232, -0.0176],\n",
       "         ...,\n",
       "         [ 0.1693, -0.2723,  0.1470,  ..., -0.1137,  0.0236, -0.0181],\n",
       "         [ 0.1688, -0.2733,  0.1481,  ..., -0.1137,  0.0231, -0.0172],\n",
       "         [ 0.1683, -0.2729,  0.1478,  ..., -0.1138,  0.0236, -0.0177]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mqa = MultiQueryAttention(32, 4)\n",
    "input = torch.rand((32, 128, 32))\n",
    "out = mqa(input)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a931032e",
   "metadata": {},
   "source": [
    "## Group Query Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91b2b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupQueryAttention(nn.Module):\n",
    "    def __init__(self, n_dim: int, n_heads: int, n_groups: int):\n",
    "        super(GroupQueryAttention, self).__init__()\n",
    "        self.n_dim = n_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = self.n_dim // self.n_heads\n",
    "        self.n_groups = n_groups\n",
    "        self.n_rep = self.n_heads // self.n_groups\n",
    "        self.q_linear = nn.Linear(self.n_dim, self.n_dim)\n",
    "        self.k_linear = nn.Linear(self.n_dim, self.n_groups * self.head_dim)\n",
    "        self.v_linear = nn.Linear(self.n_dim, self.n_groups * self.head_dim)\n",
    "        self.out_linear = nn.Linear(self.n_dim, self.n_dim)\n",
    "    \n",
    "    def repeat_kv(self, x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "        bs, seq_len, kv_heads, head_dim = x.size()\n",
    "        if n_rep == 1:\n",
    "            return x\n",
    "        else:\n",
    "            return (\n",
    "                x[:, :, :, None, :]\n",
    "                .expand(bs, seq_len, kv_heads, n_rep, head_dim)\n",
    "                .contiguous()\n",
    "                .view(bs, seq_len, kv_heads * n_rep, head_dim)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor = None):\n",
    "        bs, seq_len, n_dim = x.size()\n",
    "        xq, xk, xv = self.q_linear(x), self.k_linear(x), self.v_linear(x)\n",
    "        xq = xq.view(bs, -1, self.n_heads, self.head_dim)\n",
    "        xk, xv = xk.view(bs, -1, self.n_groups, self.head_dim), xv.view(bs, -1, self.n_groups, self.head_dim)\n",
    "        \n",
    "        xq = xq.transpose(1, 2)\n",
    "        xk, xv = self.repeat_kv(xk, self.n_rep), self.repeat_kv(xv, self.n_rep)\n",
    "        xk, xv = xk.transpose(1, 2), xv.transpose(1, 2)\n",
    "        scores = torch.matmul(xq, xk.transpose(-1, -2)) / self.head_dim ** 0.5\n",
    "        if attention_mask is not None:\n",
    "            scores.masked_fill_(attention_mask == 0, float('-inf'))\n",
    "        scores = torch.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(scores, xv)\n",
    "        out = out.transpose(1, 2).contiguous().view(bs, seq_len, n_dim)\n",
    "        out = self.out_linear(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7183f519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3260,  0.0513,  0.1795,  ...,  0.0380, -0.2347, -0.2409],\n",
       "         [-0.3239,  0.0509,  0.1806,  ...,  0.0374, -0.2339, -0.2407],\n",
       "         [-0.3254,  0.0503,  0.1798,  ...,  0.0387, -0.2335, -0.2408],\n",
       "         ...,\n",
       "         [-0.3251,  0.0519,  0.1801,  ...,  0.0378, -0.2345, -0.2399],\n",
       "         [-0.3257,  0.0500,  0.1795,  ...,  0.0373, -0.2338, -0.2403],\n",
       "         [-0.3252,  0.0513,  0.1803,  ...,  0.0380, -0.2345, -0.2408]],\n",
       "\n",
       "        [[-0.3143,  0.0495,  0.2043,  ...,  0.0436, -0.2519, -0.2410],\n",
       "         [-0.3128,  0.0491,  0.2047,  ...,  0.0431, -0.2526, -0.2423],\n",
       "         [-0.3124,  0.0496,  0.2054,  ...,  0.0433, -0.2519, -0.2411],\n",
       "         ...,\n",
       "         [-0.3127,  0.0495,  0.2062,  ...,  0.0436, -0.2523, -0.2411],\n",
       "         [-0.3135,  0.0493,  0.2042,  ...,  0.0435, -0.2527, -0.2418],\n",
       "         [-0.3135,  0.0498,  0.2055,  ...,  0.0431, -0.2530, -0.2413]],\n",
       "\n",
       "        [[-0.3214,  0.0605,  0.2003,  ...,  0.0362, -0.2619, -0.2429],\n",
       "         [-0.3227,  0.0615,  0.2004,  ...,  0.0362, -0.2615, -0.2418],\n",
       "         [-0.3217,  0.0615,  0.2005,  ...,  0.0369, -0.2619, -0.2417],\n",
       "         ...,\n",
       "         [-0.3220,  0.0598,  0.1991,  ...,  0.0374, -0.2614, -0.2418],\n",
       "         [-0.3219,  0.0602,  0.1994,  ...,  0.0375, -0.2616, -0.2422],\n",
       "         [-0.3229,  0.0599,  0.1992,  ...,  0.0370, -0.2604, -0.2423]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.3218,  0.0581,  0.2000,  ...,  0.0379, -0.2455, -0.2323],\n",
       "         [-0.3223,  0.0572,  0.2003,  ...,  0.0376, -0.2454, -0.2325],\n",
       "         [-0.3229,  0.0574,  0.1988,  ...,  0.0377, -0.2474, -0.2337],\n",
       "         ...,\n",
       "         [-0.3206,  0.0574,  0.2015,  ...,  0.0364, -0.2460, -0.2325],\n",
       "         [-0.3220,  0.0573,  0.1996,  ...,  0.0382, -0.2465, -0.2325],\n",
       "         [-0.3221,  0.0572,  0.1998,  ...,  0.0376, -0.2470, -0.2329]],\n",
       "\n",
       "        [[-0.3367,  0.0503,  0.2078,  ...,  0.0305, -0.2461, -0.2330],\n",
       "         [-0.3354,  0.0511,  0.2097,  ...,  0.0300, -0.2456, -0.2329],\n",
       "         [-0.3353,  0.0510,  0.2089,  ...,  0.0298, -0.2450, -0.2327],\n",
       "         ...,\n",
       "         [-0.3356,  0.0511,  0.2089,  ...,  0.0288, -0.2451, -0.2324],\n",
       "         [-0.3358,  0.0508,  0.2098,  ...,  0.0295, -0.2462, -0.2330],\n",
       "         [-0.3364,  0.0516,  0.2089,  ...,  0.0303, -0.2449, -0.2326]],\n",
       "\n",
       "        [[-0.3240,  0.0512,  0.1979,  ...,  0.0305, -0.2578, -0.2469],\n",
       "         [-0.3245,  0.0506,  0.1988,  ...,  0.0305, -0.2559, -0.2464],\n",
       "         [-0.3246,  0.0510,  0.1976,  ...,  0.0305, -0.2581, -0.2474],\n",
       "         ...,\n",
       "         [-0.3247,  0.0511,  0.1977,  ...,  0.0312, -0.2587, -0.2476],\n",
       "         [-0.3248,  0.0511,  0.1987,  ...,  0.0306, -0.2567, -0.2464],\n",
       "         [-0.3242,  0.0513,  0.1989,  ...,  0.0302, -0.2563, -0.2466]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gqa = GroupQueryAttention(32, 8, 4)\n",
    "input = torch.rand((32, 128, 32))\n",
    "out = gqa(input)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a637adb",
   "metadata": {},
   "source": [
    "**Why MQA/GQA accelerate inference?**\n",
    "\n",
    "Reduce KV Cache memory usage and the need to load more keys and values from kv cache."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
